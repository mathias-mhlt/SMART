{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf2b389",
   "metadata": {},
   "source": [
    "# Consistent Representation Issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c7a81",
   "metadata": {},
   "source": [
    "La cohérence de la représentation de la base de données correspond à une étude d'interprétation des données. Il s'agit en effet de déterminer si les objets sont désignés de la même manière dans toute notre base. C'est détecter lorsqu'on a plusieurs représentations pour une même valeur logique. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9459a3",
   "metadata": {},
   "source": [
    "exemple : in a feature ‘‘city’’, New York shall not be also represented as NYC or NY. Consistent representation is different from uniqueness, which focuses on ensuring that no duplicate records exist in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8b867",
   "metadata": {},
   "source": [
    "Dans mon cas, je ne repère pas les incohérences dans les valeurs numériques des données mais je me contente bien de m'intéresser à la présence d'abréviations, de raccourcis ou de redondances sémentiques dans ma base. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fa1932",
   "metadata": {},
   "source": [
    "Au début on a l'étude de la cohérence colonne par colonne mais peut être que dans un 2e temps on pourrait voir à quels facteurs la colonne est liée pour avoir une idée de regroupement possible car les autres données auront reconnu la valeur logique avec l'exploitation des données. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d55407",
   "metadata": {},
   "source": [
    "Voici les deux approches que je vais essayer d'explorer :\n",
    "- approche sémentique (basée sur la simple reconnaissance de synonymes)\n",
    "- détection des abréviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80f0ec",
   "metadata": {},
   "source": [
    "## Etude de synonymes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e8687",
   "metadata": {},
   "source": [
    "### Regroupement par thématique (abandonné)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9113124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groupes conceptuels détectés :\n",
      "['sun']\n",
      "['sky']\n",
      "['cloud']\n",
      "['banana', 'apple', 'mango']\n",
      "['vehicle']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gouineaud/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/gouineaud/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import download\n",
    "download('wordnet')\n",
    "download('omw-1.4')\n",
    "\n",
    "def get_synsets(word):\n",
    "    return wn.synsets(word)\n",
    "\n",
    "def are_similar(word1, word2, threshold=0.8):\n",
    "    synsets1 = get_synsets(word1)\n",
    "    synsets2 = get_synsets(word2)\n",
    "    max_sim = 0\n",
    "    for s1 in synsets1:\n",
    "        for s2 in synsets2:\n",
    "            sim = s1.wup_similarity(s2)\n",
    "            if sim and sim > max_sim:\n",
    "                max_sim = sim\n",
    "    return max_sim >= threshold\n",
    "\n",
    "# === Traitement progressif ===\n",
    "\n",
    "seen_groups = []  # liste de listes de mots similaires\n",
    "input_words = [\"sun\", \"sky\", \"cloud\", \"banana\", \"vehicle\", \"apple\", \"mango\"]\n",
    "\n",
    "for word in input_words:\n",
    "    matched = False\n",
    "    for group in seen_groups:\n",
    "        if any(are_similar(word, w) for w in group):\n",
    "            group.append(word)\n",
    "            matched = True\n",
    "            break\n",
    "    if not matched:\n",
    "        seen_groups.append([word])\n",
    "\n",
    "print(\"Groupes conceptuels détectés :\")\n",
    "for group in seen_groups:\n",
    "    print(group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08163f8d",
   "metadata": {},
   "source": [
    "### Détection des synonymes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37912ad",
   "metadata": {},
   "source": [
    "Le fonctionnement de l'algo de répérage des syonymes sera fondamentalement le même, les différentes méthodes explorer résideront seulement dans la manière d'obtenir les synonymes d'un mot. \n",
    "Voici la structure de notre algo\n",
    "- teste si le mot est un mot \"de base\" de notre BD, c-a-d qu'on ne prend pas pour synonyme\n",
    "    - si ce mot ne fait pas partie des mots \"de base\", on regarde si il fait partie des synonymes trouvés\n",
    "        - si il fait pas partie des synonymes, alors : \n",
    "            - on l'ajoute dans les mots de bases \n",
    "            - on ajoute ses synonymes dans les synonymes trouvés => c'est cette phase qu'il faut creuser car c'est compliqué de trouver les synonmyes d'un mot\n",
    "        - si il fait partie des synonymes trouvés, on compte +1 dans le nombre des éléments qu'il faut changer pour avoir une base cohérente\n",
    "    - si il fait partie des mots de base, on fait rien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef16d0",
   "metadata": {},
   "source": [
    "#### API synonymes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93288119",
   "metadata": {},
   "source": [
    "amélioration possible :\n",
    "- plutôt que d'enregistrer mes mots directement dans le dictionnaire, on peut les regrouper par thématique (WordNet)\n",
    "- faire une recherche http sur ce site : https://crisco4.unicaen.fr/des/synonymes et recup les 3 premiers synonymes utilisés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f9466b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "#input\n",
    "#fichier = \"words.csv\"\n",
    "#fichier = \"big_words.csv\"\n",
    "fichier = \"synonym_rich_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e07b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonyme_from_api(word):\n",
    "    url = f\"https://api.datamuse.com/words?rel_syn={word}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return [item['word'] for item in response.json()]\n",
    "    return []\n",
    "\n",
    "def synonym_from_http (word) : \n",
    "    synonymes=[]\n",
    "    ##algo trouver synonyme avec http \n",
    "    return synonymes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4071cbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeu de donnée passé en paramètres : \n",
      "           Word  Value\n",
      "0          pear     56\n",
      "1          pear      2\n",
      "2         sharp      6\n",
      "3           car     37\n",
      "4        banana     90\n",
      "..          ...    ...\n",
      "395  automobile     11\n",
      "396       sharp     22\n",
      "397       grape     86\n",
      "398        pear     51\n",
      "399         car     95\n",
      "\n",
      "[400 rows x 2 columns]\n",
      "colonnes retenues (celles qui sont reconnues pour être du texte) :  Index(['Word'], dtype='object')\n",
      "{'vehicle', 'grape', 'banana', 'apple', 'clever', 'bright', 'pear', 'fruit', 'sharp', 'car'}\n",
      "{'abrupt', 'salt', 'intense', 'sheeny', 'shiny', 'edged', 'sharply', 'glimmering', 'glistering', 'penetrating', 'happy', 'sour', 'railway car', 'agleam', 'light', 'elevator car', 'ingenious', 'glary', 'scintillant', 'piercing', 'glossy', 'carnassial', 'motorcar', 'fulgid', 'shrewd', 'machine', 'glimmery', 'cutting', 'nitid', 'buttonlike', 'glaring', 'brilliantly', 'blinding', 'scintillating', 'promising', 'acute', 'steep', 'canny', 'cagey', 'grapevine', 'railcar', 'lambent', 'pointed', 'gondola', 'opalescent', 'iridescent', 'apt', 'high', 'artful', 'acerb', 'silvern', 'glorious', 'burnished', 'unpleasant', 'acuate', 'gleaming', 'automobile', 'lustrous', 'intelligent', 'perceptive', 'shimmery', 'sharpened', 'glittering', 'malus pumila', 'radiant', 'beamy', 'sudden', 'brilliant', 'smart', 'shrill', 'silvery', 'adroit', 'keen', 'opaline', 'penetrative', 'shining', 'auto', 'reverberant', 'orchard apple tree', 'dazzling', 'astute', 'colourful', 'sunshiny', 'undimmed', 'pearlescent', 'glistening', 'scratching', 'refulgent', 'cagy', 'high-pitched', 'brightly', 'distinct', 'silver', 'beadlike', 'lurid', 'noctilucent', 'yield', 'stabbing', 'glowing', 'polished', 'glittery', 'luminous', 'railroad car', 'clear', 'lucent', 'vivid', 'glinting', 'buttony', 'tart', 'ringing', 'cable car', 'flashing', 'sparkly', 'beaming', 'fulgurating', 'sharp-worded', 'auspicious', 'blazing', 'knifelike', 'sunny', 'fulgent', 'acerbic', 'blinking', 'pear tree', 'banana tree', 'colorful', 'astringent', 'nacreous', 'cunning', 'ardent', 'pyrus communis', 'forceful', 'incisive', 'precipitous', 'discriminating', 'beady', 'crisp', 'effulgent'}\n",
      "\n",
      "Nombre de synonymes trouvés : 108 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lire le fichier\n",
    "df = pd.read_csv(fichier)\n",
    "print(\"Jeu de donnée passé en paramètres : \")\n",
    "print(df)\n",
    "\n",
    "# Récupérer uniquement les colonnes de type texte (objet = chaîne)\n",
    "text_columns = df.select_dtypes(include=\"object\").columns\n",
    "total_text_columns = len(text_columns)             #nombre de colonnes textuelles\n",
    "total_text_entries_columns = df[text_columns].size #nombre de données par colonnes\n",
    "print(\"colonnes retenues (celles qui sont reconnues pour être du texte) : \", text_columns)\n",
    "\n",
    "#init compte des valeurs à changer par synonymes dans la BD \n",
    "count_val_to_change=0\n",
    "\n",
    "\n",
    "# Boucle sur chaque ligne des colonnes texte\n",
    "for col in text_columns:\n",
    "    #intialisation des set\n",
    "    set_mot_base = set()\n",
    "    set_synonymes = set()\n",
    "    count_val_to_change=0\n",
    "    for val in df[col].dropna():  # ici on a enlever les valeurs nulles des colonnes car on ne traite pas ce cas dans ce critère\n",
    "        if val not in set_mot_base : \n",
    "            if val not in set_synonymes : \n",
    "                set_mot_base.add(val)\n",
    "                liste_syn = synonyme_from_api(val)\n",
    "                for synonyme in liste_syn : \n",
    "                    set_synonymes.add(synonyme)\n",
    "            else:\n",
    "                count_val_to_change+=1\n",
    "    print(set_mot_base)\n",
    "    print(set_synonymes)\n",
    "\n",
    "print()\n",
    "print(\"Nombre de synonymes trouvés :\",count_val_to_change,'\\n') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0100dc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de case à changer à cause des synonymes dans les colonnes uniquement textuelles : 0.73\n",
      "nombre de case à changer à cause des synonymes dans toute la base de données : 0.9325\n"
     ]
    }
   ],
   "source": [
    "text_column_inconsistency=1-1/total_text_columns*(count_val_to_change/(total_text_columns*total_text_entries_columns))\n",
    "total_synonym_inconsistency=1-1/df.size*(count_val_to_change/df.shape[1])\n",
    "\n",
    "print(\"nombre de case à changer à cause des synonymes dans les colonnes uniquement textuelles :\", text_column_inconsistency)\n",
    "print(\"nombre de case à changer à cause des synonymes dans toute la base de données :\", total_synonym_inconsistency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb5d7f",
   "metadata": {},
   "source": [
    "Plus on est proche de 1 mieux c'est :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a073c9",
   "metadata": {},
   "source": [
    "#### WordNet (abandonné)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2335f42c",
   "metadata": {},
   "source": [
    "Solution locale (donc très rapide), pas d'évolution possible mais pas très efficace pour le traitement de synonymes. Sert plutôt à cerner des concepts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71ec6a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gouineaud/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groupes de synonymes détectés :\n",
      "['smart']\n",
      "['clever']\n",
      "['intelligent']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import download\n",
    "download('wordnet')\n",
    "\n",
    "def get_lemmas(word):\n",
    "    return set(lemma.name().lower() for syn in wn.synsets(word) for lemma in syn.lemmas())\n",
    "\n",
    "# Fonction pour détecter si deux mots sont des synonymes\n",
    "def are_synonyms(word1, word2):\n",
    "    return not get_lemmas(word1).isdisjoint(get_lemmas(word2))\n",
    "\n",
    "# Traitement progressif\n",
    "seen_words = []\n",
    "synonym_groups = []\n",
    "\n",
    "input_words = [\"smart\", \"clever\", \"intelligent\"]\n",
    "\n",
    "for word in input_words:\n",
    "    found = False\n",
    "    for group in synonym_groups:\n",
    "        if any(are_synonyms(word, existing) for existing in group):\n",
    "            group.append(word)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        synonym_groups.append([word])\n",
    "\n",
    "# Résultat\n",
    "print(\"Groupes de synonymes détectés :\")\n",
    "for group in synonym_groups:\n",
    "    print(group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4259181",
   "metadata": {},
   "source": [
    "#### Embellings (abandonné)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55a0a065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-12 11:38:09.452018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groupe 1: car, automobile, vehicle\n",
      "Groupe 2: banana\n",
      "Groupe 3: fruit\n",
      "Groupe 4: truck\n",
      "Groupe 5: auto\n",
      "Groupe 6: bike\n",
      "Groupe 7: smart\n",
      "Groupe 8: clever\n",
      "Groupe 9: intelligent\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Liste de mots à tester\n",
    "words = [\"car\", \"automobile\", \"vehicle\", \"banana\", \"fruit\", \"truck\", \"auto\", \"bike\", \"smart\", \"clever\", \"intelligent\"]\n",
    "\n",
    "# Charger le modèle\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(words, convert_to_tensor=True)\n",
    "\n",
    "# Calcul des similarités cosinus\n",
    "similarities = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "# Regrouper les mots selon un seuil de similarité\n",
    "threshold = 0.85\n",
    "groups = []\n",
    "seen = set()\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    if word in seen:\n",
    "        continue\n",
    "    group = [word]\n",
    "    for j in range(len(words)):\n",
    "        if i != j and similarities[i][j] >= threshold:\n",
    "            if words[j] not in seen:\n",
    "                group.append(words[j])\n",
    "                seen.add(words[j])\n",
    "    seen.add(word)\n",
    "    groups.append(group)\n",
    "\n",
    "# Affichage\n",
    "for idx, group in enumerate(groups, 1):\n",
    "    print(f\"Groupe {idx}: {', '.join(group)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ec4198",
   "metadata": {},
   "source": [
    "Résultat pas concluant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c13c448",
   "metadata": {},
   "source": [
    "## Etude des abréviations (à faire)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
